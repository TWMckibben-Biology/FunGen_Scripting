---
title: "DGE Analyses"
output: 
  html_notebook:
   code_folding: show
author: "Amanda D. Clark"
---

## Setting Up

### Setting Up Environment

```{r}
# clear workspace
rm(list=ls(all.names=TRUE))

# Function to check for a package host on CRAN, then install (if needed) and library the package
prep_cranpack <- function (x){
if (!requireNamespace(x, quietly = TRUE)) {
  install.packages(x)
  library(x, character.only = TRUE, quietly = TRUE)
} else {
  library(x, character.only = TRUE, quietly = TRUE)
}}

# Function to check for a package host on bioconductor, then install (if needed) and library the package
prep_biocpack <- function (x){
if (!requireNamespace(x, quietly = TRUE)) {
  BiocManager::install(x)
  library(x, character.only = TRUE, quietly = TRUE)
} else {
  library(x, character.only = TRUE, quietly = TRUE)
}}

# loading list of CRAN packages
cranpacks <- c("BiocManager", "tools", "devtools", "tidyverse", "RColorBrewer", "cowplot")
invisible(lapply(cranpacks, prep_cranpack))

# loading list of Bioconductor packages
biocpacks <- c("ballgown", "RNAseq123", "edgeR", "DESeq2", "limma", "Glimma")
invisible(lapply(biocpacks, prep_biocpack))



# directory for input files
indir <- "../R_outputs/QuantPrep_Filter"

# make a directory for output files
if (! dir.exists("../R_outputs/DGE_Analyses")) {
 dir.create("../R_outputs/DGE_Analyses")
}
outdir <- "../R_outputs/DGE_Analyses"
```

### Setting up Input Files

words

I can individual load the data for each pipeline separately, and add them to a list

```{r eval=F, echo=F, collapse=T}
# reading in count data
hf_htsh <- read.csv(file.path(indir, "hard_filtered_htsh.csv"), header = T, row.names = 1 ) #Hard-filtered (see Purpose) count matrices
hf_htss <- read.csv(file.path(indir,"hard_filtered_htss.csv"), header = T, row.names = 1 )
hf_kall <- read.csv(file.path(indir,"hard_filtered_kallisto.csv"), header = T, row.names = 1 )
hf_salm <- read.csv(file.path(indir,"hard_filtered_salmon.csv"), header = T, row.names = 1 )
hf_strh <- read.csv(file.path(indir,"hard_filtered_strgtieh.csv"), header = T, row.names = 1 )
hf_strs <- read.csv(file.path(indir,"hard_filtered_strgties.csv"), header = T, row.names = 1)

sf_htsh <- read.csv(file.path(indir,"soft_filtered_htsh.csv"), header = T, row.names = 1)  #Soft-filtered count matrices
sf_htss <- read.csv(file.path(indir,"soft_filtered_htss.csv"), header = T, row.names = 1)
sf_kall <- read.csv(file.path(indir,"soft_filtered_kallisto.csv"), header = T, row.names = 1)
sf_salm <- read.csv(file.path(indir,"soft_filtered_salmon.csv"), header = T, row.names = 1)
sf_strh <- read.csv(file.path(indir,"soft_filtered_strgtieh.csv"), header = T, row.names = 1)
sf_strs <- read.csv(file.path(indir,"soft_filtered_strgties.csv"), header = T, row.names = 1)

# make list of dataframes
datlist <- list(hf_htsh=hf_htsh,hf_htss=hf_htss,hf_kall=hf_kall,hf_salm=hf_salm,
                hf_strh=hf_strh,hf_strs=hf_strs,sf_htsh=sf_htsh,sf_htss=sf_htss,
                sf_kall=sf_kall,sf_salm=sf_salm,sf_strh=sf_strh,sf_strs=sf_strs)

# adding in sample metadata
samples <- read.table("../R_inputs/samples.txt", header = T) #Read in sample table
  #make new column for treatment (control vs Experiment)
samples <- samples %>% dplyr::select(SRRID, SAMPNAME) %>%  #select the sample ID and name
  mutate(Treat = ifelse(grepl("C", samples$SAMPNAME), "Restricted", #make new column for treatment (restricted vs adlib)
                            ifelse(grepl("E", samples$SAMPNAME), "AdLib", "Error")))
## make a note to change 
head(samples)

```

Or I can generate a List object with the file names and data

```{r}
# sample metadata
samples <- read.table("../R_inputs/samples.txt", header = T) #Read in sample table
  #make new column for treatment (control vs Experiment)
samples <- samples %>% dplyr::select(SRRID, SAMPNAME) %>%  #select the sample ID and name
  mutate(Treat = ifelse(grepl("C", samples$SAMPNAME), "Restricted", #make new column for treatment (restricted vs adlib)
                            ifelse(grepl("E", samples$SAMPNAME), "AdLib", "Error")))

# generate data vector
files <- list() # empty list for file paths
count_data <- vector(mode = "list", length = 2) # empty list for data vector
files <- list.files(indir, ".csv", full.names = T) # populate list from input directory

count_data <- list(f_name = c(file_path_sans_ext(basename(files))), f_content = files %>% map(read.csv, header = T, row.names =1)) # populate list with file names, paths, and content
names(count_data$f_content) <- count_data$f_name # name matrices based on file names
```

## Purpose

At this point in our analyses, we have filtered our raw abundance data and we want to understand the differences in gene expression between our two experimental treatments (Caloric Restriction [C] vs. Ad lib [E]). As you can imagine, we are comparing our biological replicates from each treatment for each of the genes remaining after filtering. (Visualization here?)

Even if we consider our most stringently filtered data sets (hard_filtered), there is still on average X genes to compare between treatments. The more significance testing we perform (asking gene by gene - is this gene expressed differently between our C & E treatments), the more likely, by chance, that we will have false positives (differences detected when there are none) or statistically known as type I error (falsely rejecting the null that there is no difference).

Let's demonstrate this by looking at the probability of seeing a significant result by chance, based on the number of tests run at a significance level of 0.05:

```{r}
# Calculate the probability of one significant test (false positive) for a certain number of tests at a certain significance level

alpha=0.05 #significance level
tests=10 #number of significance tests

calc_falsep <- round((1 - (1 - alpha)^tests)*100,2) #calculation with conversion to percentage rounded to 2 digits 

cat("At a significance level of",alpha,"the probability of 1 in", tests,"tests being significant by chance is",calc_falsep,"%.\n")

# Visualize this calculation across different numbers of tests

falsepos <- data.frame(numbtest = c(seq(0,100,5))) %>% mutate(chanceprob = 1 - (0.95)^numbtest) #df with number of tests up to 100 and probability of 1 false positive based on the calculation above (significance level 0.05)


falseposplot <- ggplot(data = falsepos, mapping = aes(numbtest, chanceprob)) + geom_point(color = "orange", size = 3.5) + labs(x="Number of Tests", y="Chance of False Positive") + theme_minimal() #plot relationship of variables in the df above
falseposplot #check it out

```

*To think about:* This was only 100 tests (Remember we have **X** genes in our smallest comparison)! Try these calculations at different levels of significance and number of tests.

In multiple testing methodology, there is a fine balance between capturing true positives without accumulating false positives (difference found when not true/incorrectly rejecting the null) OR losing information by accumulating false negatives (not finding a difference when there is one/incorrectly NOT rejecting the null). This balance is usually found by making adjustments to p-value estimates to control false positive rates also called type I error.

Once such method is the false discovery rate (FDR), which estimates the proportion of expected false positives given the number of genes identified as being differentially expressed. Setting a cut-off for this value allows for the awareness of the estimated amount of error in the final results. Check out the Sources & Resources for more information and practice with the statistics underlying gene expression analyses.

### Sources & Resources

[Sven Schmit's blog on Multiple Hypothesis Testing (MHT)](https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/)

[Harvard Chen Bioinformatics Core Training Tutorials](https://github.com/hbctraining/DGE_workshop_salmon_online/blob/master/schedule/links-to-lessons.md)

## DGE Functions

Let's start thinking about the algorithms behind DGE programs. Here I've written functions that visualize the count data and run 3 different DGE methods that all take slightly different approaches to estimate differential gene expression. This will be a very simplified explanation, but there are more in-depth resources above to help with the underlying statistical methods used.

DGE programs aim to understand the differences in gene expression in response to, let's say, different experimental treatments. To understand TRUE differences, there are many factors that have to be accounted for, like:

1.  What does RNA-seq data looks like (How can you measure unexpected/unknown differences in expression without a base expectation)?

2.  What are the differences that will arise due to technical variation (Using different sequencers, flowcells, or RNA library prep methods)?

3.  What are the differences that will arise due to inherent biological variation (different individuals means an increase in heterogeneity)?

![Figure from Harvard Chan Bioinformatics Core Tutorial](https://hbctraining.github.io/DGE_workshop_salmon_online/img/de_variation.png)

A way to account for these complexities is by modeling the nature of RNA-seq data and the variation sourced from technical and biological heterogeneity. There are several types of models and accompanying distributions that mirror measured occurrences and/or pattern of RNA-seq data (or other data types). These models are often simpler than the complexities of real-life, but can provide a basis of expectation so that we can identify interesting deviations (not violations) from the expectations. It is amazing how close these approximations can get when you have sufficient data (and estimates of uncertainty)!

Let's think about an experiment with only technical replicates (the same pool of RNA sequenced multiple, independent times - like across multiple flow cells or lanes).

If you know the probability of a read being assigned to a specific region of the genome/transcriptome and the total number of reads, you can estimate the number of reads that will be aligned to that given region using a Poisson distribution (at really large numbers of total reads to align and a small probability of being aligned to a specific region, the Poisson matches the binomial distribution so we are just counting outcomes here).

While a Poisson distribution seems to be a good model of RNA-seq data, it usually holds only for comparisons between technical replicates where the variance should equal the mean (there should be little difference between multiple replicates of the same sample). When this is not true, that is an indication that there may be problems due to technical errors/issues.

*To think about:* What are some possible sources of variation between technical replicates?

You realistically don't have the exact same sample sequenced (unless you're interested in technical factors and have lots of money), but have samples from different biological replicates (different cultures, mice, humans, etc.) for each treatment. When you consider an experiment including different individuals or biological replicates, a Poisson distribution is no longer appropriate due to the fact that there will be overdispersion (that the variance between biological replicates will exceed the mean; i.e., the mean will not equal the variance). Modeling overdispersed counts requires adding a random effect term (pulled from a gamma distribution) that helps model the heterogeneity due to biological replicates - this is a negative binomial distribution (the smaller the dispersion in the data between biological replicates, the closer a negative binomial distribution will be to a Poisson).

Deseq2, EdgeR, and Limma Voom are similar in that they all use this negative bionomial distribution to model RNA-seq data to detect meaningful differences between groups. Above each function below, I provide some detail of the specifics for methods used in each function.

### Library Visualization Function

Quality control and check is an essential step of RNA-seq analysis. Checking your data will allow you to detect technical issues driving patterns in your data. This will help you make choices about omitting samples (if you have sufficient samples) or additional model parameters. 

```{r}
run.Vis <- function(x, y) {
  y <- y[[1]][1]
  cat("Currently visualizing libraries for pipeline:", y, "\n\n") #print which file is being processed
  colnames(x) <- c(samples$SAMPNAME) #add column names
  cat("\nColumn names:", names(x), "\n\n") #print column names
  # possible issue, this object may not be the same across programs, but that shouldn't be a problem if it's just a data format. all information going into downstream programs are the same.
  dat <- DGEList(x, group = as.factor(c(samples$Treat))) #create DGEList object, merging counts, metadata, and specifies the grouping variable for samples 
  print(dat)
  
  
  # look at raw lib size in parallel
  barplot(dat$samples$lib.size*1e-6, names = 1:10, ylab = "Library size (millions)", xlab = y) #Make barplot of the library sizes
  # Saved in the object
  b.plot = recordPlot()
  dev.off()
  
  cpm <- cpm(dat)
  lcpm <- cpm(dat, log = TRUE)
  L <- mean(dat$samples$lib.size) * 1e-6
  M <- median(dat$samples$lib.size) * 1e-6
  cat("\n", "Mean and Median Library Size in Millions:", c(L, M),"\n")
  cnttab <- table(rowSums(dat$counts == 0) == 10) 
  cnttab 
 
  # MDS Plots
  col.group <- as.factor(c(samples$Treat))
  levels(col.group) <- brewer.pal(nlevels(col.group), "Set1")
  col.group <- as.character(col.group)
  mds.plot <- plotMDS(lcpm, labels = samples$SAMPNAME, col = col.group, main = paste0("Pipeline: ", y))
  # Saved in the object
  mds.plot = recordPlot()
  dev.off()
  # Density Plots to observe filtering effects (may need to reformat output to a list and then organize them after running the function to compare hard/soft filters)
  col.group <- brewer.pal(ncol(x), "Set3")
  lcpm.cutoff <- log2(10/M + 2/L)
  plot(density(lcpm[,1]), col = col.group[1], lwd = 2, ylim = c(0,0.26), las = 2, main = y)
  for (i in 2:ncol(x)){
  den <- density(lcpm[,i])
  lines(den$x, den$y, col = col.group[i], lwd = 2)
  }
  legend("topright", samples$SAMPNAME, text.col = col.group, bty = "n")
  abline(v = lcpm.cutoff, lty = 3)
  # Saved in the object
  density.res = recordPlot()
  dev.off()
 
  # Box Plots
  dat2 <- calcNormFactors(dat, method = "TMM")
  lcpm2 <- cpm(dat2, log=TRUE)
  print(head(lcpm))
  print(head(lcpm2))
  dat$samples$norm.factors
  dat2$samples$norm.factors
  par(mfrow=c(1,2)) #getting 
  boxplot(lcpm, las = 2, col = col.group, main = "Unnormalized data", ylab = "Log-CPM")
  boxplot(lcpm2, las = 2, col = col.group, main = "Normalized data", ylab = "Log-CPM")
  box.res = recordPlot()
  dev.off()
  
  pdf(file = paste0(outdir,"/",y,"_dataExploration.pdf"))
  print(b.plot)
  print(mds.plot)
  print(density.res)
  print(box.res)
  dev.off()
}
```

### DESeq2 DGE Function

words

```{r}
run.DESeq <- function(x, y) {
  y <- y[[1]][1]
  cat("Currently proccessing:", y, "with DESeq \n") #print which file is being processed
  colnames(x) <- c(samples$SAMPNAME) #add column names
  cat("\nColumn names:", names(x), "\n\n") #print column names 
  
  dat <- DESeqDataSetFromMatrix(countData = x, colData = samples, 
                                design = ~Treat) #create DESeq object, merging counts, metadata, and specifies the predictor variable for gene counts
  print(dat)
  cat("\nResults Below: \n")
  mod <- DESeq(dat, minReplicatesForReplace = Inf) #running the DESeq function. minReplicatesForReplace=Inf prevents replacement of outlier counts
  res <- results(mod, independentFiltering = FALSE,cooksCutoff = FALSE, contrast = c("Treat", "Restricted", "AdLib"),
                 pAdjustMethod = "fdr") #store results table. skipping outlier adjustments and additional low count filtering. using a false discovery rate p-value adjustment
  print(head(res))
  print(summary(res))
  
  # make data frame output, reorder, and filter
  reslist <- list(X = res@rownames,logFC = res@listData$log2FoldChange, meanExpr = res@listData$baseMean,
                   pval = res@listData$pvalue, adj.pval = res@listData$padj)
  resdf <- as.data.frame(do.call(cbind, reslist)) %>% mutate(meanExpr = as.numeric(meanExpr), pval = as.numeric(pval),adj.pval = as.numeric(adj.pval))
  
  resOrdered <- resdf[order(as.numeric(resdf$logFC)),] #results reordered by the adjusted pvalue
  resSig <- subset(resOrdered, as.numeric(adj.pval) < 0.05)
  print(head (resSig))
  print(summary(resSig))
  
  out <- resSig
  
  write.csv(as.data.frame(out),file=paste0(outdir,"/",y,"_DESeq2.csv"), row.names = F) #write results to a new csv
  
  pdf(file = paste0(outdir,"/",y,"_DESeq.pdf"))
  DESeq2::plotMA(res)
  dev.off()
  
}
```

### edgeR DGE Function

words

```{r}
run.EdgeR <- function(x, y) {
  y <- y[[1]][1]
  cat("Currently proccessing:", y, "with edgeR \n") #print which file is being processed
  colnames(x) <- c(samples$SAMPNAME) #add column names
  cat("\nColumn names:", names(x), "\n\n") #print column names 
  
  
  dat <- DGEList(x, group = samples$Treat) #create DGEList object, merging counts, metadata, and specifies the grouping variable for samples 
  
  # est common & tagwise dispersion
  mod <- estimateCommonDisp(dat)
  mod <- estimateTagwiseDisp(mod)
  
  # perform exact test btwn caloric restriction & ad lib groups, store as 'res'
  modTest <- exactTest(mod)
  res <- topTags(modTest, n = nrow(modTest$table))
  
  # extract significant differentially expressed genes, sort, & write to csv
  resOrdered <- res$table[order(res$table$logFC),]
  resSig <- subset(resOrdered, as.numeric(FDR) < 0.05)
  print(head(resSig))
  
  out <- resSig %>% dplyr::select(logFC, logCPM, PValue, FDR) %>% dplyr::rename(meanExpr = logCPM, pval = PValue, adj.pval = FDR)
  write.csv(as.data.frame(out),file = paste0(outdir,"/",y,"_edgeR.csv")) #write results to a new csv

  cat("The number of significant DE genes is: ", nrow(resSig),"\n\n")
  
  pdf(file = paste0(outdir,"/",y,"_edgeR.pdf"))
  edgeR::plotMD.DGEExact(modTest)
  dev.off()
  
  
}
```

### LimmaVoom DGE Function

words Summarize & simplify: "What is voom doing? Counts are transformed to log2 counts per million reads (CPM), where "per million reads" is defined based on the normalization factors we calculated earlier A linear model is fitted to the log2 CPM for each gene, and the residuals are calculated A smoothed curve is fitted to the sqrt(residual standard deviation) by average expression (see red line in plot above) The smoothed curve is used to obtain weights for each gene and sample that are passed into limma along with the log2 CPMs. More details at <https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29>"

```{r}
run.LimVoo <- function(x,y) {
  y <- y[[1]][1]
  cat("Currently proccessing:", y ,"with Limma-Voom \n") #print which file is being processed by which function as a sanity check
  colnames(x) <- c(samples$SAMPNAME) #add column names to the data object
  cat("\nColumn names:", names(x), "\n\n") #print column names as a sanity check for order
  
  Treat <- c(samples$Treat)
  group=Treat
  dat <- DGEList(x, group = Treat) #create DGEList object, merging counts, metadata, and specifies the grouping variable for samples 
  #print(class(dat))
  
  # Normalization (based on the plots, is this necessary?)
  dat <- calcNormFactors(dat, method = "TMM")
  dat$samples$norm.factors
  dat
  
  mod <- model.matrix(~0 + group)
  mod
  
  varMod <- voom(dat, mod, plot = T) # Would be nice to stop and compare hard/soft filtering again here
  
  modFit <- lmFit(varMod, mod)
  #print(head(coef(modFit)))
  
  contr <- makeContrasts(groupAdLib - groupRestricted, levels = colnames(coef(modFit)))
  #print(head(contr))
  
  fitContr <- contrasts.fit(modFit, contr)
  fitContr <- eBayes(fitContr)
  
  res <- topTable(fitContr, sort.by = "logFC", n = Inf)
  print(head(res, 8)) 
  cat("Results where FDR is less than 0.01: ", length(which(res$adj.P.Val < 0.01)), "\n")
  cat("Results where FDR is less than 0.05: ", length(which(res$adj.P.Val < 0.05)), "\n")
  cat("Results where FDR is less than 0.1: ", length(which(res$adj.P.Val < 0.1)), "\n")
  
  resSig <- subset(res, as.numeric(adj.P.Val) < 0.05)
  out <- resSig %>% dplyr::select(logFC, AveExpr, P.Value, adj.P.Val) %>% dplyr::rename(meanExpr = AveExpr, pval = P.Value, adj.pval = adj.P.Val)
  print(head(out))
  
  etRes <- decideTests(fitContr)
  print(summary(etRes))
  
  write.csv(as.data.frame(out),file = paste0(outdir,"/",y,"_LimmaVoom.csv")) #write results to a new csv
  
  pdf(file = paste0(outdir,"/",y,"_LimmaVoom.pdf"))
  plotMD(fitContr, column = 1, status = etRes[,1], main = paste(colnames(fitContr)[1],y,sep = "_"), xlim = c(-0.1,20))
  varMod <- voom(dat, mod, plot = T)
  dev.off()
  
}
```

## Ballgown DGE

words Summarize & simplify: There are many ballgown specific input files that make it difficult to use the previously filtered data with these programs. Only things processed with stringtie with the for ballgown output are readily formatted for this program. These will not be run within a function.? Some resources: <https://rnabio.org/module-03-expression/0003/04/01/DE_Visualization/> <https://rstudio-pubs-static.s3.amazonaws.com/289617_cb95459057764fdfb4c42b53c69c6d3f.html> <https://davetang.org/muse/2017/10/25/getting-started-hisat-stringtie-ballgown/>

```{r eval=F, echo=F}

# We loaded our "phenotype" data in the beginning, so we don't need to repeat this step. 
# create a ballgown object for the star and hisat2 outputs; stringtie and ballgown are complementary programs

bg_star <- ballgown(dataDir = "../R_inputs/ballgown_star/", samplePattern = "SRR", pData = samples)
bg_hisat <- ballgown(dataDir = "../R_inputs/ballgown_hisat/", samplePattern = "SRR", pData = samples)

# check out the objects
class(bg_star)
class(bg_hisat)

bg_star
bg_hisat

# filtering, following previous logic for pipeline specific

bg_star_f1 <- ballgown::subset(bg_star, 
                               "rowSums(gexpr(bg_star)==0) <= 5", 
                               genomesubset=TRUE) # first filter, remove rows with 6 or more 0s
bg_star_f1
bg_star_fltrd <- ballgown::subset(bg_star_f1, 
                                  "rowSums(gexpr(bg_star_f1)) >= 21") # second filter, remove rows that sum to less than 21
bg_star_fltrd


bg_hisat_f1 <- ballgown::subset(bg_hisat, 
                                "rowSums(gexpr(bg_hisat)==0) <= 5", 
                                genomesubset=TRUE) # first filter, remove rows with 6 or more 0s
bg_hisat_f1
bg_hisat_fltrd <- ballgown::subset(bg_hisat_f1, 
                                   "rowSums(gexpr(bg_hisat_f1)) >= 21") # second filter, remove rows that sum to less than 21
bg_hisat_fltrd

# run dge analysis and output data to file (should filter by qvalue? -- what did I filter by with other tables?)
bg_star_genes <- stattest(bg_star_fltrd,
                          feature="gene",
                          covariate="Treat",
                          getFC=TRUE, meas="FPKM")
dim(bg_star_genes)
table(bg_star_genes$qval<0.05)


bg_hisat_genes <- stattest(bg_hisat_fltrd,
                          feature="gene",
                          covariate="Treat",
                          getFC=TRUE, meas="FPKM")

dim(bg_hisat_genes)
table(bg_hisat_genes$qval<0.05)

# output results
# extract significant differentially expressed genes, sort, & write to csv

bg_hisat_genes[,"de"] <- log2(bg_hisat_genes[,"fc"])
sigpi = which(bg_hisat_genes[,"pval"]<0.05)
sigp = bg_hisat_genes[sigpi,]
sigde = which(abs(sigp[,"de"]) >= 2)
sig_tn_de = sigp[sigde,]
o = order(sig_tn_de[,"qval"], -abs(sig_tn_de[,"de"]), decreasing=FALSE)
output = sig_tn_de[o,c("id","fc","pval","qval","de")]
write.csv(as.data.frame(output),file = paste0(outdir,"/","hisat_Ballgown.csv")) #write results to a new csv

bg_star_genes[,"de"] <- log2(bg_star_genes[,"fc"])
sigpi = which(bg_hisat_genes[,"pval"]<0.05)
sigp = bg_hisat_genes[sigpi,]
sigde = which(abs(sigp[,"de"]) >= 2)
sig_tn_de = sigp[sigde,]
o = order(sig_tn_de[,"qval"], -abs(sig_tn_de[,"de"]), decreasing=FALSE)
output <- sig_tn_de[o,c("id","fc","pval","qval","de")]
write.csv(as.data.frame(output),file = paste0(outdir,"/","star_Ballgown.csv")) #write results to a new csv

# visualize results

bg_star_genes$mean <- rowMeans(texpr(bg_star_fltrd))
bg_star_plot <- ggplot(bg_star_genes, aes(log2(mean), log2(fc), colour = qval<0.05)) +
  scale_color_manual(values=c("#999999", "#FF0000")) +
  geom_point() +
  geom_hline(yintercept=0)

bg_hisat_genes$mean <- rowMeans(texpr(bg_hisat_fltrd))
bg_hisat_plot <- ggplot(bg_hisat_genes, aes(log2(mean), log2(fc), colour = qval<0.05)) +
  scale_color_manual(values=c("#999999", "#FF0000")) +
  geom_point() +
  geom_hline(yintercept=0)

bg_star_plot
bg_hisat_plot

```

## Apply DGE functions

I am applying each function in loop here

```{r eval=F, echo=F}

cnt <- 1
for (i in datlist){
run.Vis(i, names(datlist)[cnt])
cnt <- cnt +1
}

cnt <- 1
for (i in datlist){
run.DESeq(i, names(datlist)[cnt])
cnt <- cnt +1
}

cnt <- 1
for (i in datlist){
run.EdgeR(i, names(datlist)[cnt])
cnt <- cnt +1
}

cnt <- 1
for (i in datlist){
run.LimVoo(i, names(datlist)[cnt])
cnt <- cnt +1
}
```

I can also map or mapply over the data set, looping over each DEseq function

```{r}
# List of functions needed to run on count matrices
funct <- c( "run.Vis","run.DESeq", "run.EdgeR", "run.LimVoo")

# Apply each function to each count data set in List object
for (func in funct) {
mapply(func, count_data$f_content, count_data$f_name)
}

```
